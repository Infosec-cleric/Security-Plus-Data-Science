{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from scipy.sparse import hstack\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_pickle('data.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Train Test Split:- </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(data.drop('intrusion_type', axis=1), data['intrusion_type'], stratify=data['intrusion_type'], test_size=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data\n",
      "(109189, 41)\n",
      "(109189,)\n",
      "====================\n",
      "Test data\n",
      "(36397, 41)\n",
      "(36397,)\n"
     ]
    }
   ],
   "source": [
    "print('Train data')\n",
    "print(X_train.shape)\n",
    "print(Y_train.shape)\n",
    "print('='*20)\n",
    "print('Test data')\n",
    "print(X_test.shape)\n",
    "print(Y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Vectorizing Categorical features using one-hot encoding:- </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Categorical features in our dataset are:- 'protocol_type', 'service', and 'flag'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5> Protocol_type:- </h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Protocol types are: ['udp', 'tcp', 'icmp']\n"
     ]
    }
   ],
   "source": [
    "protocol = list(X_train['protocol_type'].values)\n",
    "protocol = list(set(protocol))\n",
    "print('Protocol types are:', protocol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot = CountVectorizer(vocabulary=protocol, binary=True)\n",
    "train_protocol = one_hot.fit_transform(X_train['protocol_type'].values)\n",
    "test_protocol = one_hot.transform(X_test['protocol_type'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 0]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(109189, 3)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(train_protocol[1].toarray())\n",
    "train_protocol.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5> Service:- </h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Service types are:\n",
      " ['telnet', 'gopher', 'printer', 'name', 'supdup', 'pop_2', 'link', 'X11', 'exec', 'netstat', 'rje', 'ntp_u', 'nnsp', 'imap4', 'discard', 'nntp', 'pop_3', 'sunrpc', 'http_443', 'tim_i', 'whois', 'eco_i', 'private', 'hostnames', 'red_i', 'domain', 'netbios_dgm', 'vmnet', 'kshell', 'pm_dump', 'Z39_50', 'finger', 'tftp_u', 'uucp', 'iso_tsap', 'ctf', 'csnet_ns', 'IRC', 'ftp', 'other', 'ftp_data', 'remote_job', 'mtp', 'shell', 'efs', 'ecr_i', 'ldap', 'urh_i', 'klogin', 'login', 'time', 'courier', 'uucp_path', 'sql_net', 'echo', 'bgp', 'urp_i', 'systat', 'domain_u', 'ssh', 'auth', 'smtp', 'daytime', 'netbios_ns', 'netbios_ssn', 'http']\n"
     ]
    }
   ],
   "source": [
    "service = list(X_train['service'].values)\n",
    "service = list(set(service))\n",
    "print('Service types are:\\n', service)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/iti/.local/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:1322: UserWarning: Upper case characters found in vocabulary while 'lowercase' is True. These entries will not be matched with any documents\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "one_hot = CountVectorizer(vocabulary=service, binary=True)\n",
    "train_service = one_hot.fit_transform(X_train['service'].values)\n",
    "test_service = one_hot.transform(X_test['service'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "print(train_service[100].toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(109189, 66)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_service.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5> Flag:- </h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "flag types are: ['SH', 'RSTO', 'S3', 'S0', 'SF', 'RSTOS0', 'S1', 'RSTR', 'REJ', 'S2', 'OTH']\n"
     ]
    }
   ],
   "source": [
    "flag = list(X_train['flag'].values)\n",
    "flag = list(set(flag))\n",
    "print('flag types are:', flag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot = CountVectorizer(binary=True)\n",
    "one_hot.fit(X_train['flag'].values)\n",
    "train_flag = one_hot.transform(X_train['flag'].values)\n",
    "test_flag = one_hot.transform(X_test['flag'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 0 0 0 0 0 0 1 0]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(109189, 11)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(test_flag[3000].toarray())\n",
    "train_flag.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.drop(['protocol_type','service','flag'], axis=1, inplace=True)\n",
    "X_test.drop(['protocol_type','service','flag'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Applying Standardisation on the continuous features of our dataset:- </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_scaling(X_train, X_test, feature_name):\n",
    "    \n",
    "    '''\n",
    "    This function performs standardisation on the features\n",
    "    '''\n",
    "    \n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    scaler1 = scaler.fit_transform(X_train[feature_name].values.reshape(-1,1))\n",
    "    scaler2 = scaler.transform(X_test[feature_name].values.reshape(-1,1))\n",
    "    \n",
    "    return scaler1, scaler2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "duration1, duration2 = feature_scaling(X_train, X_test, 'duration')\n",
    "src_bytes1, src_bytes2 = feature_scaling(X_train, X_test, 'src_bytes')\n",
    "dst_bytes1, dst_bytes2 = feature_scaling(X_train, X_test, 'dst_bytes')\n",
    "wrong_fragment1, wrong_fragment2 = feature_scaling(X_train, X_test, 'wrong_fragment')\n",
    "urgent1, urgent2 = feature_scaling(X_train, X_test, 'urgent')\n",
    "hot1, hot2 = feature_scaling(X_train, X_test, 'hot')\n",
    "num_failed_logins1, num_failed_logins2 = feature_scaling(X_train, X_test, 'num_failed_logins')\n",
    "num_compromised1, num_compromised2 = feature_scaling(X_train, X_test, 'num_compromised')\n",
    "root_shell1, root_shell2 = feature_scaling(X_train, X_test, 'root_shell')\n",
    "su_attempted1, su_attempted2 = feature_scaling(X_train, X_test, 'su_attempted')\n",
    "num_root1, num_root2 = feature_scaling(X_train, X_test, 'num_root')\n",
    "num_file_creations1, num_file_creations2 = feature_scaling(X_train, X_test, 'num_file_creations')\n",
    "num_shells1, num_shells2 = feature_scaling(X_train, X_test, 'num_shells')\n",
    "num_access_files1, num_access_files2 = feature_scaling(X_train, X_test, 'num_access_files')\n",
    "srv_count1, srv_count2 = feature_scaling(X_train, X_test, 'srv_count')\n",
    "serror_rate1, serror_rate2 = feature_scaling(X_train, X_test, 'serror_rate')\n",
    "srv_serror_rate1, srv_serror_rate2 = feature_scaling(X_train, X_test, 'srv_serror_rate')\n",
    "rerror_rate1, rerror_rate2 = feature_scaling(X_train, X_test, 'rerror_rate')\n",
    "srv_rerror_rate1, srv_rerror_rate2 = feature_scaling(X_train, X_test, 'srv_rerror_rate')\n",
    "same_srv_rate1, same_srv_rate2 = feature_scaling(X_train, X_test, 'same_srv_rate')\n",
    "diff_srv_rate1, diff_srv_rate2 = feature_scaling(X_train, X_test, 'diff_srv_rate')\n",
    "srv_diff_host_rate1, srv_diff_host_rate2 = feature_scaling(X_train, X_test, 'srv_diff_host_rate')\n",
    "dst_host_count1, dst_host_count2 = feature_scaling(X_train, X_test, 'dst_host_count')\n",
    "dst_host_srv_count1, dst_host_srv_count2 = feature_scaling(X_train, X_test, 'dst_host_srv_count')\n",
    "dst_host_same_srv_rate1, dst_host_same_srv_rate2= feature_scaling(X_train, X_test, 'dst_host_same_srv_rate')\n",
    "dst_host_diff_srv_rate1, dst_host_diff_srv_rate2 = feature_scaling(X_train, X_test, 'dst_host_diff_srv_rate')\n",
    "dst_host_same_src_port_rate1, dst_host_same_src_port_rate2 = feature_scaling(X_train, X_test, 'dst_host_same_src_port_rate')\n",
    "dst_host_srv_diff_host_rate1, dst_host_srv_diff_host_rate2 = feature_scaling(X_train, X_test, 'dst_host_srv_diff_host_rate')\n",
    "land1, land2 = np.array([X_train['land'].values]), np.array([X_test['land'].values])\n",
    "logged_in1, logged_in2 = np.array([X_train['logged_in'].values]), np.array([X_test['logged_in'].values])\n",
    "is_host_login1, is_host_login2 = np.array([X_train['is_host_login'].values]), np.array([X_test['is_host_login'].values])\n",
    "is_guest_login1, is_guest_login2 = np.array([X_train['is_guest_login'].values]), np.array([X_test['is_guest_login'].values])\n",
    "count1, count2 = feature_scaling(X_train, X_test, 'count')\n",
    "dst_host_srv_rerror_rate1, dst_host_srv_rerror_rate2 = feature_scaling(X_train, X_test, 'dst_host_srv_rerror_rate')\n",
    "dst_host_rerror_rate1, dst_host_rerror_rate2 = feature_scaling(X_train, X_test, 'dst_host_rerror_rate')\n",
    "dst_host_srv_serror_rate1, dst_host_srv_serror_rate2 = feature_scaling(X_train, X_test, 'dst_host_srv_serror_rate')\n",
    "dst_host_serror_rate1, dst_host_serror_rate2 = feature_scaling(X_train, X_test, 'dst_host_serror_rate')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Merging categorical and continuous features:- </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_1 = hstack((duration1, train_protocol, train_service, train_flag, src_bytes1,\n",
    "       dst_bytes1, land1.T, wrong_fragment1, urgent1, hot1,\n",
    "       num_failed_logins1, logged_in1.T, num_compromised1, root_shell1,\n",
    "       su_attempted1, num_root1, num_file_creations1, num_shells1,\n",
    "       num_access_files1, is_host_login1.T,\n",
    "       is_guest_login1.T, count1, srv_count1, serror_rate1,\n",
    "       srv_serror_rate1, rerror_rate1, srv_rerror_rate1, same_srv_rate1,\n",
    "       diff_srv_rate1, srv_diff_host_rate1, dst_host_count1,\n",
    "       dst_host_srv_count1, dst_host_same_srv_rate1,\n",
    "       dst_host_diff_srv_rate1, dst_host_same_src_port_rate1,\n",
    "       dst_host_srv_diff_host_rate1, dst_host_serror_rate1,\n",
    "       dst_host_srv_serror_rate1, dst_host_rerror_rate1,\n",
    "       dst_host_srv_rerror_rate1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(109189, 117)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_1 = hstack((duration2, test_protocol, test_service, test_flag, src_bytes2,\n",
    "       dst_bytes2, land2.T, wrong_fragment2, urgent2, hot2,\n",
    "       num_failed_logins2, logged_in2.T, num_compromised2, root_shell2,\n",
    "       su_attempted2, num_root2, num_file_creations2, num_shells2,\n",
    "       num_access_files2, is_host_login2.T,\n",
    "       is_guest_login2.T, count2, srv_count2, serror_rate2,\n",
    "       srv_serror_rate2, rerror_rate2, srv_rerror_rate2, same_srv_rate2,\n",
    "       diff_srv_rate2, srv_diff_host_rate2, dst_host_count2,\n",
    "       dst_host_srv_count2, dst_host_same_srv_rate2,\n",
    "       dst_host_diff_srv_rate2, dst_host_same_src_port_rate2,\n",
    "       dst_host_srv_diff_host_rate2, dst_host_serror_rate2,\n",
    "       dst_host_srv_serror_rate2, dst_host_rerror_rate2,\n",
    "       dst_host_srv_rerror_rate2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(36397, 117)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(X_train_1,'X_train_1.pkl')\n",
    "joblib.dump(X_test_1,'X_test_1.pkl')\n",
    "X_train_1 = joblib.load('X_train_1.pkl')\n",
    "X_test_1 = joblib.load('X_test_1.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(Y_train,'Y_train.pkl')\n",
    "joblib.dump(Y_test,'Y_test.pkl')\n",
    "Y_train = joblib.load('Y_train.pkl')\n",
    "Y_test = joblib.load('Y_test.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
